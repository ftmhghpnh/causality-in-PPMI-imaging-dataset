{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e015a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387aa320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seriesIdentifier</th>\n",
       "      <th>Mfg Model</th>\n",
       "      <th>mode</th>\n",
       "      <th>preprocessed_3_path_jpeg</th>\n",
       "      <th>frame_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103294</td>\n",
       "      <td>TrioTim</td>\n",
       "      <td>train</td>\n",
       "      <td>/w/284/gzk/result/JPEG_files/103294/0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103294</td>\n",
       "      <td>TrioTim</td>\n",
       "      <td>train</td>\n",
       "      <td>/w/284/gzk/result/JPEG_files/103294/1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103294</td>\n",
       "      <td>TrioTim</td>\n",
       "      <td>train</td>\n",
       "      <td>/w/284/gzk/result/JPEG_files/103294/2.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103294</td>\n",
       "      <td>TrioTim</td>\n",
       "      <td>train</td>\n",
       "      <td>/w/284/gzk/result/JPEG_files/103294/3.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103294</td>\n",
       "      <td>TrioTim</td>\n",
       "      <td>train</td>\n",
       "      <td>/w/284/gzk/result/JPEG_files/103294/4.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seriesIdentifier Mfg Model   mode  \\\n",
       "0            103294   TrioTim  train   \n",
       "1            103294   TrioTim  train   \n",
       "2            103294   TrioTim  train   \n",
       "3            103294   TrioTim  train   \n",
       "4            103294   TrioTim  train   \n",
       "\n",
       "                    preprocessed_3_path_jpeg  frame_num  \n",
       "0  /w/284/gzk/result/JPEG_files/103294/0.jpg          0  \n",
       "1  /w/284/gzk/result/JPEG_files/103294/1.jpg          1  \n",
       "2  /w/284/gzk/result/JPEG_files/103294/2.jpg          2  \n",
       "3  /w/284/gzk/result/JPEG_files/103294/3.jpg          3  \n",
       "4  /w/284/gzk/result/JPEG_files/103294/4.jpg          4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpeg_csv_path = '/w/246/gzk/PPMI/codes/first_model_jpeg_csv_info.csv'\n",
    "df = pd.read_csv(jpeg_csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d50661",
   "metadata": {},
   "outputs": [],
   "source": [
    "seriesIdentifier_lists = df['seriesIdentifier'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e63c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "max_frame_per_subj = {}\n",
    "for subj in seriesIdentifier_lists:\n",
    "    tmp = df[df['seriesIdentifier'] == subj]\n",
    "    print(tmp['frame_num'].max())\n",
    "    max_frame_per_subj[subj] = tmp['frame_num'].max()\n",
    "    break\n",
    "### do not need, we have already pad the images and all they have 255 frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6564e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class EncoderVGG(nn.Module):\n",
    "    '''Encoder of image based on the architecture of VGG-16 with batch normalization.\n",
    "\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "\n",
    "    '''\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "\n",
    "        self.encoder = self._encodify_(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2a47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encodify_(self, encoder):\n",
    "        '''Create list of modules for encoder based on the architecture in VGG template model.\n",
    "\n",
    "        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling\n",
    "        indices from the corresponding pooling operation in the encoder. In VGG template, these indices\n",
    "        are not returned. Hence the need for this method to extent the pooling operations.\n",
    "\n",
    "        Args:\n",
    "            encoder : the template VGG model\n",
    "\n",
    "        Returns:\n",
    "            modules : the list of modules that define the encoder corresponding to the VGG model\n",
    "\n",
    "        '''\n",
    "        modules = nn.ModuleList()\n",
    "        for module in encoder.features:\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,\n",
    "                                          stride=module.stride,\n",
    "                                          padding=module.padding,\n",
    "                                          return_indices=True)\n",
    "                modules.append(module_add)\n",
    "            else:\n",
    "                modules.append(module)\n",
    "\n",
    "        return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "165e14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    '''Execute the encoder on the image input\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): image tensor\n",
    "\n",
    "    Returns:\n",
    "        x_code (Tensor): code tensor\n",
    "        pool_indices (list): Pool indices tensors in order of the pooling modules\n",
    "\n",
    "    '''\n",
    "    pool_indices = []\n",
    "    x_current = x\n",
    "    for module_encode in self.encoder:\n",
    "        output = module_encode(x_current)\n",
    "\n",
    "        ## If the module is pooling, there are two outputs, the second the pool indices\n",
    "        if isinstance(output, tuple) and len(output) == 2:\n",
    "            x_current = output[0]\n",
    "            pool_indices.append(output[1])\n",
    "        else:\n",
    "            x_current = output\n",
    "\n",
    "    return x_current, pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed0f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(nn.Module):\n",
    "    '''Decoder of code based on the architecture of VGG-16 with batch normalization.\n",
    "\n",
    "    Args:\n",
    "        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder\n",
    "\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_code\n",
    "    channels_out = 3\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "\n",
    "        self.decoder = self._invert_(encoder)\n",
    "\n",
    "    def _invert_(self, encoder):\n",
    "        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder\n",
    "\n",
    "        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose\n",
    "        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder\n",
    "        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization\n",
    "        and activation, and a maxpooling in encoder is turned into unpooling.\n",
    "\n",
    "        Args:\n",
    "            encoder (ModuleList): the encoder\n",
    "\n",
    "        Returns:\n",
    "            decoder (ModuleList): the decoder obtained by \"inversion\" of encoder\n",
    "        '''\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,\n",
    "                          'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.ConvTranspose2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "        ## Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "\n",
    "        return nn.ModuleList(modules_transpose)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fc835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
